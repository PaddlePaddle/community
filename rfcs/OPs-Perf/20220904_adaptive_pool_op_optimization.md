# Adaptive Pooling OP性能优化设计文档


| 基本信息                                                     | 内容                                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------- |
| 提交作者<input type="checkbox" class="rowselector hidden">   | OuyangChao   |
| 提交时间<input type="checkbox" class="rowselector hidden">   | 2022-09-04 |
| 版本号                                                 | V1.0  |
| 依赖飞桨版本<input type="checkbox" class="rowselector hidden">| PaddleDevelop|
| 文件名                    | 20220904_adaptive_pooling_op_optimization.md<br> |


# 1 背景与意义
## 1.1 飞桨现状
目前Paddle中的Adaptive Pooling OP是和常规Pooling OP共用一套cuda kernel，且优化策略仅考虑了1维的线程设置，转用2维或者3维的线程配置可能减少计算规模。

## 1.2 业内方案调研

- device: GeForce RTX 3080 (Compute Capability: 8.6)
- data_type: float32
- data_format: NCHW
- 这里的时间只包含了前向时的kernel，后续补充反向
- diff中的`-`表示paddle比pytorch更慢，`+`表示paddle比pytorch更快
- paddle的benchmark只包含了case 3，这里补充了一些作为测试

| Case No. | input\_shape        | output\_shape | pooling\_type | PyTorch(ms) | Paddle(ms) | diff      |
| -------- | ------------------- | ------------- | ------------- | ----------- | ---------- | --------- |
| 0        | \[128,64,112,112\]  | \[56,56\]     | AVG           | 0.7523      | 5.7803     | \-668.35% |
| 1        | \[128,512,7,7\]     | \[1,1\]       | AVG           | 0.0379      | 0.034      | 10.29%    |
| 2        | \[128,2048,7,7\]    | \[1,1\]       | AVG           | 0.138       | 0.1239     | 10.22%    |
| 3        | \[4,2048,64,128\]   | \[32,32\]     | AVG           | 0.4332      | 2.1554     | \-397.55% |
| 4        | \[128,64,224,224\]  | \[112,112\]   | AVG           | 3.1858      | 21.3287    | \-569.49% |
| 5        | \[128,128,112,112\] | \[56,56\]     | AVG           | 1.4968      | 11.4142    | \-662.57% |
| 6        | \[128,256,56,56\]   | \[28,28\]     | AVG           | 0.8102      | 6.5266     | \-705.55% |
| 7        | \[128,512,28,28\]   | \[14,14\]     | AVG           | 0.7937      | 3.361      | \-323.46% |
| 8        | \[128,512,14,14\]   | \[7,7\]       | AVG           | 0.4306      | 0.9218     | \-114.07% |
| 9        | \[128,64,112,112\]  | \[56,56\]     | MAX           | 1.1501      | 5.7488     | \-399.85% |
| 10       | \[128,512,7,7\]     | \[1,1\]       | MAX           | 0.5491      | 0.0882     | 83.94%    |
| 11       | \[128,2048,7,7\]    | \[1,1\]       | MAX           | 2.1786      | 0.3053     | 85.99%    |
| 12       | \[4,2048,64,128\]   | \[32,32\]     | MAX           | 0.5393      | 2.1714     | \-302.63% |
| 13       | \[128,64,224,224\]  | \[112,112\]   | MAX           | 4.852       | 21.5729    | \-344.62% |
| 14       | \[128,128,112,112\] | \[56,56\]     | MAX           | 2.3236      | 11.4789    | \-394.01% |
| 15       | \[128,256,56,56\]   | \[28,28\]     | MAX           | 1.0566      | 6.6417     | \-528.59% |
| 16       | \[128,512,28,28\]   | \[14,14\]     | MAX           | 0.7561      | 3.3837     | \-347.52% |
| 17       | \[128,512,14,14\]   | \[7,7\]       | MAX           | 0.4058      | 0.9256     | \-128.09% |

## 1.3 对比分析
- 从上面表格来看，只有在`output_shape=[1, 1]`时paddle才优于pytorch
- pytorch采用的是2d线程配置，而paddle采用的是1d线程配置，可能在index的除法和取余计算时更加耗时
- pytorch采用的是float计算输入的start和end索引，而paddle采用的是double，这里的[指令吞吐](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#arithmetic-instructions)差异也对最终性能造成较大影响，计算能力为8.6时float和double的指令吞吐差64倍

# 2 设计方案与性能预期

## 2.1 关键模块与性能提升点
- 输入的start和end索引改为float计算，提升指令吞吐
- 将kernel优化为2d线程配置，减少index的除法和取余计算
- 由于目前baseline比pytorch差几倍以上，预计优化后可以在大部分case上追平pytorch

## 2.2 Host端计算流程
根据输出的shape进行2d线程配置

## 2.4 Device端计算流程
与原来逻辑相同，改变的是根据2d的线程配置来重新计算输入输出的索引

# 3 测试和验收的考量

参考：[算子性能优化验收标准](http://agroup.baidu.com/paddle-perf/md/article/4892913)

完成Adaptive Pooling OP 开发后，Paddle与Pytorch的性能对比效果如下，达到了预期性能提升效果：

| Case No. | input\_shape        | output\_shape | pooling\_type | PyTorch(ms) | Paddle(ms) | diff    |
| -------- | ------------------- | ------------- | ------------- | ----------- | ---------- | ------- |
| 0        | \[128,64,112,112\]  | \[56,56\]     | AVG           | 0.7523      | 0.7378     | 1.93%   |
| 1        | \[128,512,7,7\]     | \[1,1\]       | AVG           | 0.0379      | 0.034      | 10.29%  |
| 2        | \[128,2048,7,7\]    | \[1,1\]       | AVG           | 0.138       | 0.1239     | 10.22%  |
| 3        | \[4,2048,64,128\]   | \[32,32\]     | AVG           | 0.4332      | 0.4335     | \-0.07% |
| 4        | \[128,64,224,224\]  | \[112,112\]   | AVG           | 3.1858      | 3.1628     | 0.72%   |
| 5        | \[128,128,112,112\] | \[56,56\]     | AVG           | 1.4968      | 1.4634     | 2.23%   |
| 6        | \[128,256,56,56\]   | \[28,28\]     | AVG           | 0.8102      | 0.7727     | 4.63%   |
| 7        | \[128,512,28,28\]   | \[14,14\]     | AVG           | 0.7937      | 0.4043     | 49.06%  |
| 8        | \[128,512,14,14\]   | \[7,7\]       | AVG           | 0.4306      | 0.1139     | 73.55%  |
| 9        | \[128,64,112,112\]  | \[56,56\]     | MAX           | 1.1501      | 0.7381     | 35.82%  |
| 10       | \[128,512,7,7\]     | \[1,1\]       | MAX           | 0.5491      | 0.0882     | 83.94%  |
| 11       | \[128,2048,7,7\]    | \[1,1\]       | MAX           | 2.1786      | 0.3053     | 85.99%  |
| 12       | \[4,2048,64,128\]   | \[32,32\]     | MAX           | 0.5393      | 0.4328     | 19.75%  |
| 13       | \[128,64,224,224\]  | \[112,112\]   | MAX           | 4.852       | 3.1088     | 35.93%  |
| 14       | \[128,128,112,112\] | \[56,56\]     | MAX           | 2.3236      | 1.472      | 36.65%  |
| 15       | \[128,256,56,56\]   | \[28,28\]     | MAX           | 1.0566      | 0.7602     | 28.05%  |
| 16       | \[128,512,28,28\]   | \[14,14\]     | MAX           | 0.7561      | 0.3932     | 48.00%  |
| 17       | \[128,512,14,14\]   | \[7,7\]       | MAX           | 0.4058      | 0.1105     | 72.77%  |


# 4 可行性分析和排期规划

时间和开发排期规划，主要milestone

| No. | 开发内容 | 预期时间 |
|---|---|---|
| 1 | 理清Paddle中OP设计思路，同类产品中最佳设计方案  | 2022-09-04 |
| 2 | 完成开发文档设计  | 2022-09-10 |
| 3 | 完成代码开发工作，并通过线程CI测试([PR45959](https://github.com/PaddlePaddle/Paddle/pull/45959)) | 2022-09-30 |


# 5 影响面
- 对于常规的Pooling OP：目前Paddle中的Adaptive Pooling OP是和常规Pooling OP共用一套cuda kernel，为了降低影响面，单独写一个cuda kernel支持Adaptive Pooling OP
- 对于`output_shape=[1, 1]`时的adaptive average pooling：由于在优化前`output_shape=[1, 1]`paddle已经优于pytorch，且adaptive average pooling运行的是reduce kernel，所以也没有进行修改，不会对这种配置下的op有影响
- 对于`output_shape=[1, 1]`时的adaptive max pooling：由于在优化前`output_shape=[1, 1]`paddle已经优于pytorch，而这种情况下采用2d线程配置后的发现性能变差了，因此这种情况也运行原来的逻辑，不会对这种配置下的op有影响

# 名词解释

# 附件及参考资料
[1]. [OP Benchmark使用指南](https://github.com/PaddlePaddle/benchmark/blob/master/api/README.md)
