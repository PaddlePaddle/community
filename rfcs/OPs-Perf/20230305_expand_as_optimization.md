# æ ‡é¢˜

æ ‡é¢˜å¦‚ï¼šTranspose OPæ€§èƒ½ä¼˜åŒ–è®¾è®¡æ–‡æ¡£
| åŸºæœ¬ä¿¡æ¯                                                   | å†…å®¹                                                         |
| ---------------------------------------------------------  | ------------------------------------------------------------ |
| æäº¤ä½œè€…<input type="checkbox" class="rowselector hidden">   | [Timber-Ye](https://github.com/Timber-Ye)ã€[BrianQian1999](https://github.com/BrianQian1999)                                               |
| æäº¤æ—¶é—´<input type="checkbox" class="rowselector hidden">   | 2023-03-05                                                   |
| ç‰ˆæœ¬å·                                                      | V1.0                                   |
| ä¾èµ–é£æ¡¨ç‰ˆæœ¬<input type="checkbox" class="rowselector hidden">| åŸºäºPaddleDevelopç‰ˆæœ¬å¼€å‘                      |
| æ–‡ä»¶å                                                       | 20230305_expand_as_op_optimization.md<br> |


# 1 èƒŒæ™¯ä¸æ„ä¹‰

ç›®å‰ Paddle å†… `expand_as` å‰å‘å’Œåå‘ç®—å­çš„ GPU å®ç°é‡‡ç”¨ Eigen ç»„åˆçš„æ¨¡å¼ï¼Œç¼ºå°‘ GPU Kernelï¼Œæ€§èƒ½ç›¸å¯¹ä¸è¶³ï¼Œå¸Œæœ›å®ç°é«˜æ€§èƒ½çš„ GPU è®¡ç®— Kernelï¼Œä¸º Paddle ä¼˜åŒ– `expand_as` op åœ¨ GPU ä¸Šçš„è®¡ç®—æ€§èƒ½ã€‚

## 1.1 é£æ¡¨ç°çŠ¶

é£æ¡¨æ¡†æ¶ç°æœ‰çš„expand_aså‰å‘ç®—å­çš„å®ç°è¿‡ç¨‹ä¸ºï¼šï¼ˆ1ï¼‰é¦–å…ˆç¡®å®šæ¯ä¸€ç»´å°†è¢«æ‰©å±•çš„æ¬¡æ•°ï¼›ï¼ˆ2ï¼‰ç›´æ¥å€ŸåŠ©Eigenåº“çš„å¹¿æ’­æ–¹æ³•ï¼Œè°ƒç”¨`funcs::EigenBroadcast`ï¼š
``````c++
funcs::EigenBroadcast<std::decay_t<decltype(place)>, T, Rank>::Eval(place, y, x0, bcast_dims)
``````
è€Œè¿™ä¸€è¿‡ç¨‹å·²ç»èƒ½å¤Ÿç”±Eigenåº“åœ¨GPUä¸Šå®ç°ã€‚

åå‘ç®—å­çš„å®ç°è¿‡ç¨‹ä¸ä¹‹ç±»ä¼¼ï¼šï¼ˆ1ï¼‰ç¡®å®šéœ€è¦è¿›è¡Œæ±‚å’ŒReductionçš„ç»´åº¦ï¼›ï¼ˆ2ï¼‰å€ŸåŠ©Eigenåº“ï¼Œå®ç°`funcs::EigenBroadcastGrad`ï¼š
```c++
funcs::EigenBroadcastGrad<std::decay_t<decltype(place)>, T, Dims>::Eval(place, x_grad, out_grad0, reduce_dims, reshape_dims);
```
å…¶å†…éƒ¨è°ƒç”¨äº†Eigenåº“ä¸­é’ˆå¯¹å¼ é‡çš„`reshape`ä»¥åŠ`sum`æ–¹æ³•ï¼š
```c++
out.device(dev) =
        in.reshape(reshape_dims).sum(reduce_dims).reshape(out.dimensions());
```
é¦–å…ˆå¯¹è¢«æ‰©å±•åçš„é«˜ç»´å¼ é‡è¿›è¡Œreshapeï¼Œä»¥ä¾¿åç»­åœ¨æŒ‡å®šç»´åº¦ä¸Šè¿›è¡Œæ±‚å’Œï¼Œæœ€åå†å°†ç»“æœreshapeåˆ°å¸Œæœ›è¾“å‡ºçš„å½¢çŠ¶ï¼Œä»¥æ­¤è¾¾åˆ°çº¦å½’é™ç»´çš„ç›®çš„ã€‚

ä¸‹è¡¨åˆ—å‡ºäº†paddleæ¡†æ¶çš„expand_asç®—å­åœ¨[OP Benchmark](https://github.com/PaddlePaddle/benchmark/tree/master/api/tests_v2)ä¸­å„ç§caseåœºæ™¯ä¸‹çš„OPæ€§èƒ½æ•°æ®ï¼ˆæµ‹è¯•ç¯å¢ƒï¼šTesla V100-32G, CUDA 11.2ï¼‰ã€‚

| Case | Data type | src_shape    | dst_shape      | Paddle Forward (ms) |  Paddle Backward (ms) |   Total(ms)  |
| ---- | --------- | ------------ | -------------- | ----------          | ----------------------|--------------|
| 0    | float32   | [1785, 1]    | [1785, 128]    | 0.074236            | 0.172566              | 0.246802     |
| 1    | float32   | [5, 1, 1]    | [5, 128, 128]  | 0.082833            | 3.594770              | 3.677603     |
| 2    | float32   | [32, 807, 1] | [32, 807, 807] | 0.427489            | 1.107112              | 1.532601     |
| 3    | float16   | [1785, 1]    | [1785, 128]    | 0.049622            | 0.147476              | 0.197098     |
| 4    | float16   | [5, 1, 1]    | [5, 128, 128]  | 0.051206            | 3.039735              | 3.090941     |
| 5    | float16   | [32, 807, 1] | [32, 807, 807] | 0.407556            | 0.980826              | 1.388382     |

## 1.2 ä¸šå†…æ–¹æ¡ˆè°ƒç ”

### 1.2.1 Tensorflow

tensorflowä¸­çš„`tf.tile` å¯ä»¥ç”¨æ¥åœ¨å¤šä¸ªç»´åº¦ä¸Šé‡å¤input tensorï¼Œè¯¥æ–¹æ³•ä¸expand_asåŠŸèƒ½è¿‘ä¼¼ã€‚å…¶è¿‡ç¨‹å¤§è‡´æ˜¯ç”±é«˜ç»´å‘ä½ç»´è¿›è¡Œé€’å½’æ‰©å±•ï¼Œæ¯ä¸€æ¬¡æ‰©å±•å®é™…ä¸Šéƒ½æ˜¯åœ¨è¿›è¡Œä¸€æ¬¡æ•°æ®æ‹·è´ ï¼ˆ[æºç é“¾æ¥ï¼šğŸ”—](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/kernels/tile.cc#L88-L97) ï¼‰ã€‚

`tf.tile`ä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„å¼ é‡æ¥ä¿å­˜å¤åˆ¶åçš„å¼ é‡ï¼Œç”±äºå¤åˆ¶æ“ä½œæ¶‰åŠå¤§é‡æ•°æ®çš„è¯»å†™IOè¿ç®—ï¼Œè®¡ç®—ä»£ä»·ç›¸å¯¹è¾ƒé«˜ã€‚

### 1.2.2 Pytorch

Pytorchä¸­å­˜åœ¨expandç®—å­ï¼Œå…¶å‰å‘è¿‡ç¨‹åŒPaddleæ¡†æ¶ç°æœ‰æ–¹æ³•ä¸€è‡´ï¼Œè¢«è§†ä½œä¸€æ¬¡Broadcastï¼ˆ[è°ƒç”¨ä½ç½®ï¼šğŸ”—](https://github.com/pytorch/pytorch/blob/master/caffe2/operators/expand_op.h#L58-L67)ï¼‰ï¼Œä½†Pytorchè‡ªèº«å®ç°äº†å¹¿æ’­è¿‡ç¨‹çš„GPU Kernelï¼ˆ[æºç é“¾æ¥ï¼šğŸ”—](https://github.com/pytorch/pytorch/blob/master/caffe2/utils/math_gpu.cu#L2781-L2804)ï¼‰ï¼š
``````c++
template <typename T, int D>
__global__ void BroadcastCUDAKernel(
    const int Y_size,
    const SimpleArray<int, D> X_strides,
    const SimpleArray<FIXED_DIVISOR, D> Y_dims,
    const T alpha,
    const T* X,
    T* Y) {
  CUDA_1D_KERNEL_LOOP(Y_index, Y_size) {
    int X_index = 0;
    int Y_index_val = Y_index;
#pragma unroll
    for (int i = D - 1; i >= 0; --i) {
      int d;
      FIXED_DIVISOR_DIV_MOD(Y_dims.data[i], Y_index_val, &Y_index_val, &d);
      X_index += d * X_strides.data[i];
    }
#if __CUDA_ARCH__ >= 350 || defined(USE_ROCM)
    Y[Y_index] = __ldg(X + X_index) * alpha;
#else
    Y[Y_index] = X[X_index] * alpha;
#endif
  }
}
``````
è¯¥Kernelå®ç°åŸºäºElementWiseæ–¹å¼ï¼Œå…³é”®è¿‡ç¨‹æ˜¯æ‰¾åˆ°`Y_index`ä¸`X_index`ä¹‹é—´çš„æ˜ å°„å…³ç³»ã€‚

Expandç®—å­çš„åå‘è¿‡ç¨‹åŒæ ·åŸºäºçº¦å½’æ±‚å’Œ(ReduceSum)çš„æ–¹æ³•ï¼ˆ[è°ƒç”¨ä½ç½®ï¼šğŸ”—](https://github.com/pytorch/pytorch/blob/master/caffe2/operators/expand_op.h#L108-L116)ï¼‰ï¼Œå…¶GPU Kernelå®ç°ï¼ˆ[æºç é“¾æ¥ï¼šğŸ”—](https://github.com/pytorch/pytorch/blob/b8dfb45ac282a48764c192ac7d27b7d80eed8b2b/caffe2/utils/math/reduce.cu#L104-L135)ï¼‰ï¼š
``````c++
template <typename T, class Reducer, int D>
__global__ void ReduceTensorCUDAKernel(
    const int inner_size,
    const SimpleArray<int, D> X_strides,
    const SimpleArray<int, D> Y_dims,
    const Reducer reducer,
    const T init,
    const T alpha,
    const T* X,
    T* Y) {
  __shared__ typename BlockReduce<T>::TempStorage temp_storage;
  const int x = blockIdx.x;
  T val = init;
  for (int y = threadIdx.x; y < inner_size; y += blockDim.x) {
    int X_index = 0;
    int Y_index = x * inner_size + y;
#pragma unroll
    for (int d = D - 1; d >= 0; --d) {
      X_index += Y_index % Y_dims.data[d] * X_strides.data[d];
      Y_index /= Y_dims.data[d];
    }
#if __CUDA_ARCH__ >= 350 || defined(USE_ROCM)
    val = reducer(val, __ldg(X + X_index));
#else
    val = reducer(val, X[X_index]);
#endif
  }
  val = BlockReduce<T>(temp_storage).Reduce(val, reducer);
  if (threadIdx.x == 0) {
    Y[x] = val * alpha;
  }
}
``````

è¯¥Kernelå®ç°åŒæ ·åŸºäºElementWiseæ–¹å¼ï¼Œä½†ç”±äº`Y_index`åˆ° `X_index`çš„æ˜ å°„ä¸ºä¸€å¯¹å¤šï¼Œéœ€è¦ç”³è¯·ä¸€å—shared memoryæ¥è®°å½•`Y_index`æ‰€å¯¹åº”çš„æ‰€æœ‰`X_index`ä¸Šçš„æ•°æ®ï¼Œå¹¶åœ¨æœ€åå¯¹è¿™å—å…±äº«å†…å­˜è¿›è¡Œæ±‚å’Œï¼Œæœ€ç»ˆèµ‹å€¼ç»™`Y_index`æ‰€åœ¨ä½ç½®ã€‚

å¦å¤–é’ˆå¯¹ç‰¹æ®Šçš„æƒ…å†µï¼Œpytorchä¸­è¿˜ç‰¹åˆ«ç¼–å†™äº†RowwiseReduceï¼ˆ[æºç é“¾æ¥ï¼šğŸ”—](https://github.com/pytorch/pytorch/blob/b8dfb45ac282a48764c192ac7d27b7d80eed8b2b/caffe2/utils/math/reduce.cu#L25-L47)ï¼‰ã€ColwiseReduceï¼ˆ[æºç é“¾æ¥ï¼šğŸ”—](https://github.com/pytorch/pytorch/blob/b8dfb45ac282a48764c192ac7d27b7d80eed8b2b/caffe2/utils/math/reduce.cu#L49-L72)ï¼‰ä»¥åŠBothEndsReduceï¼ˆ[æºç é“¾æ¥ï¼šğŸ”—](https://github.com/pytorch/pytorch/blob/b8dfb45ac282a48764c192ac7d27b7d80eed8b2b/caffe2/utils/math/reduce.cu#L74-L102)ï¼‰ä¸‰ä¸ªç‰¹æ®Šçš„Kernel å®ç°ã€‚


## 1.3 å¯¹æ¯”åˆ†æ

é™¤äº†Paddleæ¡†æ¶ä»¥å¤–ï¼Œ[OP Benchmark](https://github.com/PaddlePaddle/benchmark/tree/master/api/tests_v2)ä¸­è¿˜æœ‰é’ˆå¯¹Tensorflowçš„é™æ€å›¾æµ‹è¯•è„šæœ¬ï¼Œä¸‹è¡¨ç»™å‡ºçš„æ˜¯Tensorflowæ¡†æ¶ä¸‹ExpandAsç®—å­åœ¨å„ç±»caseä¸­çš„æ€§èƒ½ï¼ˆæµ‹è¯•ç¯å¢ƒï¼šTesla V100-32G, CUDA 11.2ï¼‰ï¼š

| Case | Data type | src_shape    | dst_shape      | Tensorflow forward (ms) | Tensorflow backward (ms) | Total(ms)                |
| ---- | --------  | ---------    | ------------   | --------------          | -----------------------  | ------------------------ |
| 0    | float32   | [1785, 1]    | [1785, 128]    | 0.150479                | 0.159827                 | 0.310306                 |
| 1    | float32   | [5, 1, 1]    | [5, 128, 128]  | 0.104476                | 0.108868                 | 0.213345                 |
| 2    | float32   | [32, 807, 1] | [32, 807, 807] | 9.223847                | 9.212913                 | 18.436761                |
| 3    | float16   | [1785, 1]    | [1785, 128]    | 0.042221                | 0.044698                 | 0.086919                 |
| 4    | float16   | [5, 1, 1]    | [5, 128, 128]  | 0.024973                | 0.031283                 | 0.056257                 |
| 5    | float16   | [32, 807, 1] | [32, 807, 807] | 5.511609                | 5.298775                 | 10.810385                |

åœ¨case 2å’Œcase 5å½“ä¸­ï¼ŒPaddleæ¯”Tensorflowå¿«è¿‘10å€ï¼›è€Œåœ¨case 1å’Œcase 4ä¸­ï¼ŒTensorflowçš„æ€§èƒ½åˆæœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚å¦å¤–ï¼Œå½“æ•°æ®ç±»å‹ä»`float32`å˜ä¸º`float16`åï¼ŒTensorflowç®—å­çš„æ€§èƒ½æœ‰æ˜æ˜¾çš„æå‡ï¼Œè€Œç›¸æ¯”ä¹‹ä¸‹ï¼Œæ•°æ®ç±»å‹å¯¹Paddleç®—å­æ€§èƒ½çš„å½±å“ä¸å¤§ã€‚

ç‰¹åˆ«å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨case 1ä¸­Tensorflowåå‘ç®—å­æ¯”Paddleå¿«30å€ä»¥ä¸Šï¼Œcase 4åˆ™æ›´æ˜¯å¿«äº†è¿‘100å€ã€‚é’ˆå¯¹Paddleæ¡†æ¶åå‘ç®—å­åœ¨è¿™ä¸¤ä¸ªcaseä¸­çš„ä¸ä½³è¡¨ç°ï¼Œæˆ‘ä»¬è¿›è¡Œäº†è¿›ä¸€æ­¥æµ‹è¯•ï¼ˆæµ‹è¯•ç¯å¢ƒï¼šTesla V100-32G, CUDA 11.2ï¼‰ï¼š

| Case | Data type | src_shape    | dst_shape      | Paddle forward (ms) | Paddle backward (ms) | Total(ms)        |
| ---- | --------- | ------------ | -------------- | --------------------| ---------------------| -----------------|
| 6    | float32   | [16, 1, 1]   | [16, 807, 807] | 0.271686            | 254.208616           | 254.480303       |
| 7    | float32   | [32, 1, 1]   | [32, 256, 256] | 0.097565            | 18.539683            | 18.637249        |

ç»¼ä¸Šå¯è§ï¼Œæ— è®ºå‰å‘è¿˜æ˜¯åå‘ç®—å­ï¼ŒPaddleä¸Tensorflowç›¸æ¯”è¾ƒå‡å„æœ‰ä¼˜åŠ£ã€‚ä½†æ˜¯ï¼ŒTensorflowä¸­å‰å‘åå‘ç®—å­çš„æ€§èƒ½å·®è·ä¸å¤§ï¼Œè€Œåœ¨Paddleä¸­ï¼Œå‰å‘ç®—å­çš„æ€§èƒ½é€šå¸¸è¦æ˜æ˜¾å¥½äºåå‘ç®—å­ï¼Œä¹Ÿå°±æ˜¯è¯´**Paddleçš„åå‘ç®—å­æœ‰å¾ˆå¤§çš„ä¼˜åŒ–ç©ºé—´**ã€‚æ‰€é™ç»´æ•°ç©ºé—´è¶Šå¤§ï¼Œè¿›è¡Œçº¦å½’æ±‚å’Œçš„æ•°æ®é‡ä¹Ÿå°±è¶Šå¤§ï¼Œè¿™åº”è¯¥æ˜¯å¯¼è‡´Paddleåå‘ç®—å­æ€§èƒ½å­˜åœ¨å¦‚æ­¤å·®å¼‚çš„ä¸»è¦åŸå› ã€‚

# 2 è®¾è®¡æ–¹æ¡ˆä¸æ€§èƒ½é¢„æœŸ

è®¡åˆ’æ›¿ä»£Paddleä¸­æ‰€ä½¿ç”¨çš„Eigenåº“ï¼Œå€Ÿé‰´Pytorchï¼Œé‡‡ç”¨ElementWiseæ–¹å¼æ¥ä¼˜åŒ–expand aså‰å‘åå‘opã€‚

## 2.1 å…³é”®æ¨¡å—ä¸æ€§èƒ½æå‡ç‚¹

æ–°å¢æ¨¡å—åŒ…å«ExpandAsç®—å­å‰å‘ä»¥åŠåå‘å„è‡ªçš„GPU Kernelï¼Œç€é‡æå‡åå‘è¿‡ç¨‹çš„è¿ç®—æ•ˆç‡ã€‚

## 2.2 å‰å‘ä¼˜åŒ–

ExpandAsç®—å­çš„å‰å‘è¿‡ç¨‹åŸºæœ¬ç±»ä¼¼å¹¿æ’­æœºåˆ¶ï¼Œå³åœ¨éœ€è¦è¿›è¡Œæ‰©å±•çš„ç»´åº¦ä¸Šè¿›è¡Œæ•°æ®æ‹·è´ã€‚ä»å¹¶è¡Œç¼–ç¨‹ElementWiseçš„è§’åº¦æ¥çœ‹ï¼Œå¯ä»¥ä»¤æ¯ä¸€ä¸ªçº¿ç¨‹å¤„ç†è¾“å‡ºå¼ é‡ä¸Šçš„æ¯ä¸€ä¸ªå…ƒç´ ï¼Œé€šè¿‡Indexçš„æ˜ å°„å…³ç³»æ‰¾å‡ºè¯¥å…ƒç´ åœ¨è¾“å…¥å¼ é‡ä¸Šçš„å¯¹åº”ä½ç½®ï¼Œç„¶åè¯»å–æ•°æ®å®Œæˆèµ‹å€¼å³å¯ã€‚

## 2.2.1 Hostç«¯è®¡ç®—æµç¨‹

Hostç«¯ä¸»è¦æ˜¯å‡†å¤‡Indexæ˜ å°„æ‰€éœ€è¦çš„æ•°æ®ï¼Œä¸»è¦åŒ…æ‹¬è¾“å…¥ã€è¾“å‡ºå¼ é‡çš„ç»´æ•°ã€æ¯ä¸€ç»´çš„å°ºå¯¸ã€æ¯ä¸€ç»´çš„æ­¥é•¿ç­‰ã€‚åœ¨è¿™ä¹‹åè¿›è¡ŒKernel Launchå³å¯ã€‚

``````c++
...
auto x_shape_dim = x.dims(); // è¾“å…¥å¼ é‡åœ¨æ¯ä¸€ç»´ä¸Šçš„å°ºå¯¸
auto x_stride_dim = phi::stride(x_shape_dim); // è¾“å…¥å¼ é‡åœ¨æ¯ä¸€ç»´ä¸Šçš„æ­¥é•¿

for(int i=0; i<rank; i++){
    h_indexInfo[i] = target_shape[target_rank-i-1]; // target shape
    h_indexInfo[i + rank] = x_shape_dim[rank-i-1] == 1 ? 0 : x_stride_dim[rank-i-1]; // input stride
}
...

// kernel launch
ExpandAsForward<T>
      <<<grid_dim, block_dim, 0, stream>>>(in_data,
                                           out_data,
                                           rank,
                                           out_numel,
                                           d_indexInfo);
``````
## 2.2.2 Deviceç«¯è®¡ç®—æµç¨‹

Deviceç«¯è®¡ç®—æµç¨‹ä¸»è¦ä¸ºä»¥ElementWiseæ–¹å¼å¯»æ‰¾in_idxå’Œout_idxä¹‹é—´çš„æ˜ å°„å…³ç³»ã€‚æŸä¸ªçº¿ç¨‹åœ¨ç¡®å®šå¯¹åº”çš„ç´¢å¼•ä¹‹åï¼ŒæŒ‰ç…§ç´¢å¼•å»global memoryå½“ä¸­çš„è¾“å…¥å¼ é‡ä¸­å–å€¼ï¼Œç„¶åèµ‹å€¼ç»™è¾“å‡ºå¼ é‡çš„å¯¹åº”ä½ç½®ã€‚

å‰å‘GPU Kernelå½“ä¸­çš„å„ä¸ªçº¿ç¨‹ä¹‹é—´å®Œå…¨ç‹¬ç«‹ï¼Œæ— éœ€ç”³è¯·å…±äº«å†…å­˜ã€‚

``````c++
int in_idx, out_idx;

CUDA_KERNEL_LOOP_TYPE(idx, N, int64_t) {
    in_idx = idx;
    out_idx = f(in_idx);
    out_tensor[out_idx] = in_tensor[in_idx];
}

``````

## 2.3 åå‘ä¼˜åŒ–

ExpandAsç®—å­çš„åå‘è¿‡ç¨‹åŸºæœ¬ç±»ä¼¼çº¦å½’æ±‚å’Œè¿‡ç¨‹ï¼Œå³å°†æ‰€è¦è¿›è¡Œreduceçš„ç»´åº¦ä¸Šçš„æ‰€æœ‰å…ƒç´ è¿›è¡Œæ±‚å’Œã€‚ä»å¹¶è¡Œç¼–ç¨‹ElementWiseçš„è§’åº¦æ¥çœ‹ï¼Œå¯ä»¥ä»¤æ¯ä¸€ä¸ªThread Blockå¤„ç†è¾“å‡ºå¼ é‡ä¸Šçš„æ¯ä¸€ä¸ªå…ƒç´ ï¼Œé€šè¿‡Indexçš„æ˜ å°„å…³ç³»æ‰¾å‡ºè¯¥å…ƒç´ åœ¨è¾“å…¥å¼ é‡ä¸Šæ‰€å¯¹åº”çš„æ‰€æœ‰ä½ç½®ï¼Œç„¶ååˆ©ç”¨Paddleå†…å°è£…å¥½çš„Warpçº§æ“ä½œï¼Œè°ƒç”¨æ¥å£å¿«é€Ÿè¿›è¡ŒBlockå†…çš„å…¨éƒ¨æ•°æ®çš„æ±‚å’Œï¼Œæœ€åå®Œæˆèµ‹å€¼å³å¯ã€‚

## 2.2.1 Hostç«¯è®¡ç®—æµç¨‹

åŒæ ·çš„ï¼Œåå‘çš„Hostç«¯ä¸»è¦æ˜¯å‡†å¤‡Indexæ˜ å°„æ‰€éœ€è¦çš„æ•°æ®ï¼ŒåŒ…æ‹¬è¾“å…¥ã€è¾“å‡ºå¼ é‡çš„ç»´æ•°ã€æ¯ä¸€ç»´çš„å°ºå¯¸ã€æ¯ä¸€ç»´çš„æ­¥é•¿ç­‰ã€‚åœ¨è¿™ä¹‹åè¿›è¡ŒKernel Launchå³å¯ã€‚

## 2.2.2 Deviceç«¯è®¡ç®—æµç¨‹

Deviceç«¯è®¡ç®—æµç¨‹ä¸»è¦ä¸ºä»¥ElementWiseæ–¹å¼å¯»æ‰¾in_idxå’Œout_idxä¹‹é—´çš„æ˜ å°„å…³ç³»ã€‚ç”±äºåå‘å±äºReductionSumè¿‡ç¨‹ï¼Œå› æ­¤å¯è°ƒç”¨Paddleå°è£…å¥½çš„CUDAå·¥å…·ï¼Œå¿«é€Ÿæ±‚å–Blockå†…æ•°æ®ä¹‹å’Œï¼Œç„¶åå°†ç»“æœèµ‹å€¼ç»™è¾“å‡ºå¼ é‡çš„å¯¹åº”ä½ç½®ã€‚

``````c++
int in_idx = blockIdx.x, out_idx;
T val = 0;

for(int i = threadIdx.x; i < acc_N; i += blockDim.x){
    out_idx = f(in_idx, i);
    tmp += out_grad[out_idx];
}

__syncthreads();
T result = funcs::BlockReduceSum<T>(val, FULL_MASK);

if(threadIdx.x == 0) in_grad[in_idx] = result;

``````

# 3 æµ‹è¯•å’ŒéªŒæ”¶çš„è€ƒé‡

å‚è€ƒï¼š[ç®—å­æ€§èƒ½ä¼˜åŒ–éªŒæ”¶æ ‡å‡†](http://agroup.baidu.com/paddle-perf/md/article/4892913)


# 4 å¯è¡Œæ€§åˆ†æå’Œæ’æœŸè§„åˆ’

æ—¶é—´å’Œå¼€å‘æ’æœŸè§„åˆ’ï¼Œä¸»è¦milestone

| No. | å¼€å‘å†…å®¹ | é¢„æœŸæ—¶é—´ |
|---|---|---|
| 1 | ç†æ¸…Paddleä¸­OPè®¾è®¡æ€è·¯ï¼ŒåŒç±»äº§å“ä¸­æœ€ä½³è®¾è®¡æ–¹æ¡ˆ  | 2023-03-05 |
| 2 | å®Œæˆä¼˜åŒ–è®¾è®¡æ–‡æ¡£  | 2023-03-05 |
| 3 | expand_asä¼˜åŒ–å®ç°  | 2023-03-10 |
| 3 | å®Œæˆä»£ç å¼€å‘å·¥ä½œï¼Œå¹¶é€šè¿‡çº¿ç¨‹CIæµ‹è¯• | 2023-03-15 |



# 5 å½±å“é¢

éœ€è¦è¿›ä¸€æ­¥è®¨è®ºçš„é—®é¢˜ï¼Œå¼€æ”¾æ€§é—®é¢˜ï¼Œæœ‰äº‰è®®é—®é¢˜ï¼›å¯¹å…¶ä»–æ¨¡å—æ˜¯å¦æœ‰å½±å“ã€‚


# é™„ä»¶åŠå‚è€ƒèµ„æ–™

[1]. [OP Benchmarkä½¿ç”¨æŒ‡å—](https://github.com/PaddlePaddle/benchmark/blob/master/api/README.md)
