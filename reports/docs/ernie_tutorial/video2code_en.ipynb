{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Video to Code: Automating Code Generation with ERNIE 4.5-VL\n",
    "\n",
    "### 1. Project Overview\n",
    "\n",
    "This tutorial details and implements an end-to-end workflow for the automated conversion of video-based programming tutorials into executable front-end web code. The core of this process involves leveraging a large multimodal model for the visual comprehension of video content, which is then used to generate structured code.\n",
    "\n",
    "The workflow comprises the following key technical steps:\n",
    "1.  **Video Ingestion**: Downloading the source video file from a specified URL.\n",
    "2.  **Visual Information Extraction**: Pre-processing the video file by sampling frames at a fixed interval and encoding the image data.\n",
    "3.  **Segmented Content Comprehension**: Grouping the extracted frame sequences into chunks and making parallel calls to the ERNIE 4.5-VL model API to obtain structured text descriptions for each video segment.\n",
    "4.  **Aggregation and Code Generation**: Consolidating the descriptive text from all segments into a comprehensive summary of the entire video. This summary serves as the context for a final call to the ERNIE 4.5-VL model to generate the complete HTML code.\n",
    "5.  **Result Presentation**: Saving the generated code to an HTML file and automatically opening it in a local browser for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Core Technology: An Overview of ERNIE 4.5-VL\n",
    "\n",
    "The core of this workflow is the ERNIE 4.5 Vision-Language Model (VLM), which possesses the following characteristics highly relevant to this task:\n",
    "\n",
    "*   **Multimodal Heterogeneous MoE Architecture**: The model is pre-trained jointly on both text and visual modalities. This design enables it to effectively capture cross-modal information, which is critical for accurately understanding both the code text and the corresponding visual styles presented in the video frames.\n",
    "*   **State-of-the-Art Visual Understanding**: ERNIE 4.5-VL demonstrates exceptional performance on benchmarks related to visual perception, document analysis, and chart comprehension. This capability is fundamental to the model's ability to accurately identify syntax, structure, and visual layouts from video frames.\n",
    "*   **Open Source Protocol**: The ERNIE 4.5 model family is released under the Apache 2.0 license, facilitating its use in technical research and application development.\n",
    "\n",
    "This tutorial utilizes the visual analysis and code generation capabilities of ERNIE 4.5-VL to perform the conversion task from unstructured video to structured code.\n",
    "![ERNIE 4.5-VL Performance Comparison](ernie_4.5_vl_performance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Environment and Dependency Installation\n",
    "\n",
    "Executing this workflow requires the installation of Python libraries for video downloading, image processing, and API interaction.\n",
    "\n",
    "Run the following cell to install all necessary dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple/\n",
      "Requirement already satisfied: yt-dlp in /Users/lizhijun/miniconda3/envs/agent/lib/python3.11/site-packages (2025.10.14)\n",
      "Requirement already satisfied: opencv-python in /Users/lizhijun/miniconda3/envs/agent/lib/python3.11/site-packages (4.12.0.88)\n",
      "Requirement already satisfied: openai in /Users/lizhijun/miniconda3/envs/agent/lib/python3.11/site-packages (1.108.2)\n",
      "Requirement already satisfied: tqdm in /Users/lizhijun/miniconda3/envs/agent/lib/python3.11/site-packages (4.67.1)\n",
      "Requirement already satisfied: numpy<2.3.0,>=2 in /Users/lizhijun/miniconda3/envs/agent/lib/python3.11/site-packages (from opencv-python) (2.2.6)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/lizhijun/miniconda3/envs/agent/lib/python3.11/site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/lizhijun/miniconda3/envs/agent/lib/python3.11/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/lizhijun/miniconda3/envs/agent/lib/python3.11/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/lizhijun/miniconda3/envs/agent/lib/python3.11/site-packages (from openai) (0.10.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/lizhijun/miniconda3/envs/agent/lib/python3.11/site-packages (from openai) (2.11.9)\n",
      "Requirement already satisfied: sniffio in /Users/lizhijun/miniconda3/envs/agent/lib/python3.11/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/lizhijun/miniconda3/envs/agent/lib/python3.11/site-packages (from openai) (4.14.1)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/lizhijun/miniconda3/envs/agent/lib/python3.11/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/lizhijun/miniconda3/envs/agent/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/lizhijun/miniconda3/envs/agent/lib/python3.11/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/lizhijun/miniconda3/envs/agent/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/lizhijun/miniconda3/envs/agent/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/lizhijun/miniconda3/envs/agent/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/lizhijun/miniconda3/envs/agent/lib/python3.11/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to install all required libraries\n",
    "!pip install yt-dlp opencv-python openai tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Library Imports and Parameter Configuration\n",
    "\n",
    "This section imports the required Python modules and defines global parameters, including API credentials and the target video URL.\n",
    "\n",
    "**Instructions:**\n",
    "-   Replace the value of the `API_KEY` variable with your Baidu AI Studio Access Token.\n",
    "-   The `VIDEO_URL` is preset with an example link. You may replace it with a URL to another front-end development tutorial for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import base64\n",
    "import yt_dlp\n",
    "import shutil\n",
    "import webbrowser\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# --- Configuration Center ---\n",
    "\n",
    "# 1. API Credential Configuration\n",
    "# !!! IMPORTANT: Replace with your own Baidu ERNIE API KEY !!!\n",
    "API_KEY = \"YOUR_API_KEY\"\n",
    "BASE_URL = \"https://aistudio.baidu.com/llm/lmapi/v3\"\n",
    "MODEL_NAME = \"ernie-4.5-turbo-vl\"\n",
    "\n",
    "# 2. Task Parameter Configuration\n",
    "VIDEO_URL = \"https://www.bilibili.com/video/BV1xQ4y167xr/\" # <-- Specify the target video URL here\n",
    "MAX_CONCURRENT_REQUESTS = 4  # Number of concurrent API requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Core Function Definitions\n",
    "\n",
    "The various stages of the workflow are encapsulated into separate functions to promote modularity and code clarity.\n",
    "\n",
    "- **`download_video`**: Downloads the video file from the specified URL.\n",
    "- **`extract_frames`**: Reads the video file to extract and encode the image frame sequence.\n",
    "- **`process_chunk_with_retry`**: Calls the model API to process the frame sequence of a single video chunk, including retry logic for network requests.\n",
    "- **`aggregate_and_generate_webpage`**: Consolidates the analysis results from all chunks and calls the model API to generate the final web page code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_video(video_url, output_dir=\"temp_video\"):\n",
    "    \"\"\"\n",
    "    Downloads a video from the given URL using yt-dlp.\n",
    "    Parameters:\n",
    "        video_url (str): The URL of the video.\n",
    "        output_dir (str): A temporary directory to store the downloaded video.\n",
    "    Returns:\n",
    "        str: The local file path of the video if successful, otherwise None.\n",
    "    \"\"\"\n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.makedirs(output_dir)\n",
    "    print(f\">>> Downloading video: {video_url}\")\n",
    "    \n",
    "    ydl_opts = {\n",
    "        'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best',\n",
    "        'outtmpl': os.path.join(output_dir, 'video.%(ext)s'),\n",
    "        'quiet': True,\n",
    "        'no_warnings': True,\n",
    "        'merge_output_format': 'mp4'\n",
    "    }\n",
    "    try:\n",
    "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "            ydl.download([video_url])\n",
    "        for file in os.listdir(output_dir):\n",
    "            if file.startswith(\"video\"):\n",
    "                print(\"‚úÖ Video downloaded successfully.\")\n",
    "                return os.path.join(output_dir, file)\n",
    "        print(\"‚ùå Video download failed: Downloaded file not found.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Video download failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_frames(video_path, interval_sec=1):\n",
    "    \"\"\"\n",
    "    Extracts image frames from a video file at a specified interval.\n",
    "    Parameters:\n",
    "        video_path (str): The local path to the video file.\n",
    "        interval_sec (int): The time interval in seconds for frame extraction.\n",
    "    Returns:\n",
    "        list: A list of frame chunks, where each chunk contains 30 Base64-encoded image strings.\n",
    "    \"\"\"\n",
    "    print(\">>> Processing video and extracting frames...\")\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(\"‚ùå Error: Could not open video file.\")\n",
    "        return []\n",
    "        \n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    if fps == 0: fps = 30 \n",
    "\n",
    "    chunk_frames, chunks, frame_count = [], [], 0\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        \n",
    "        if frame_count % int(fps * interval_sec) == 0:\n",
    "            height, width = frame.shape[:2]\n",
    "            scale = 512 / height\n",
    "            resized_frame = cv2.resize(frame, (int(width * scale), 512))\n",
    "            \n",
    "            _, buffer = cv2.imencode('.jpg', resized_frame, [int(cv2.IMWRITE_JPEG_QUALITY), 60])\n",
    "            frame_b64 = base64.b64encode(buffer).decode('utf-8')\n",
    "            chunk_frames.append(frame_b64)\n",
    "            \n",
    "            if len(chunk_frames) == 30:\n",
    "                chunks.append(chunk_frames)\n",
    "                chunk_frames = []\n",
    "        frame_count += 1\n",
    "    \n",
    "    if chunk_frames:\n",
    "        chunks.append(chunk_frames)\n",
    "        \n",
    "    cap.release()\n",
    "    print(f\"‚úÖ Frame extraction complete. Video divided into {len(chunks)} chunks.\")\n",
    "    return chunks\n",
    "\n",
    "def process_chunk_with_retry(client, chunk_index, frames_b64, max_retries=3):\n",
    "    \"\"\"\n",
    "    Processes a single video chunk using the ERNIE API with a retry mechanism.\n",
    "    Parameters:\n",
    "        client (OpenAI): The API client instance.\n",
    "        chunk_index (int): The index of the chunk.\n",
    "        frames_b64 (list): A list of Base64-encoded image strings for the chunk.\n",
    "        max_retries (int): The maximum number of retry attempts.\n",
    "    Returns:\n",
    "        tuple: A tuple containing the chunk index and the model's text description.\n",
    "    \"\"\"\n",
    "    prompt = f\"This is a segment from a web development video tutorial (one screenshot per second). Focus on the code on screen and the final web page style. Describe in detail the HTML structure, CSS styling code, or JS interaction logic shown in this segment.\"\n",
    "    \n",
    "    content = [{\"type\": \"text\", \"text\": prompt}]\n",
    "    for f in frames_b64:\n",
    "        content.append({\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{f}\"}})\n",
    "        \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=MODEL_NAME,\n",
    "                messages=[{\"role\": \"user\", \"content\": content}],\n",
    "                temperature=0.1, max_tokens=1024\n",
    "            )\n",
    "            return chunk_index, response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                tqdm.write(f\"‚ùå Chunk {chunk_index+1} failed after {max_retries} attempts: {e}\")\n",
    "            time.sleep(2)\n",
    "    return chunk_index, \"\"\n",
    "\n",
    "def aggregate_and_generate_webpage(client, summaries):\n",
    "    \"\"\"\n",
    "    Aggregates summaries from all chunks and calls the API to generate the final webpage code.\n",
    "    Parameters:\n",
    "        client (OpenAI): The API client instance.\n",
    "        summaries (dict): A dictionary of chunk indices and their text descriptions.\n",
    "    Returns:\n",
    "        str: The complete generated HTML code.\n",
    "    \"\"\"\n",
    "    print(\"\\n>>> Aggregating content and generating final code...\")\n",
    "    full_summary = \"\\n\".join([f\"Summary of Segment {i+1}: {s}\" for i, s in sorted(summaries.items()) if s])\n",
    "    \n",
    "    final_prompt = f\"\"\"\n",
    "    You are an expert front-end engineer. Based on the following segmented summaries extracted from a programming video tutorial, write a single, complete HTML file to reproduce the final result shown in the video.\n",
    "\n",
    "    **Segmented Video Summaries:**\n",
    "    ---\n",
    "    {full_summary}\n",
    "    ---\n",
    "\n",
    "    **Instructions:**\n",
    "    1.  Your code must be strictly based on the HTML structure, CSS styles, and functionality mentioned in the summaries.\n",
    "    2.  Ignore any descriptions in the summaries that are not related to programming.\n",
    "    3.  If the summary information is incomplete, use your professional knowledge to reasonably complete the code to form a fully functional webpage.\n",
    "    4.  All HTML and CSS (within a `<style>` tag) must be in a single file.\n",
    "    5.  Return only the raw HTML code, starting with `<!DOCTYPE html>`, without any explanations or markdown tags.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL_NAME,\n",
    "        messages=[{\"role\": \"user\", \"content\": final_prompt}],\n",
    "        stream=True, temperature=0.2, top_p=0.8\n",
    "    )\n",
    "    \n",
    "    html_code = \"\"\n",
    "    print(\">>> The model is now generating code:\\n\")\n",
    "    for chunk in response:\n",
    "        if hasattr(chunk.choices[0].delta, \"content\") and chunk.choices[0].delta.content:\n",
    "            char = chunk.choices[0].delta.content\n",
    "            print(char, end=\"\", flush=True)\n",
    "            html_code += char\n",
    "    return html_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Main Program Execution\n",
    "\n",
    "Running the following cell will initiate the entire automated workflow. The script will use the `VIDEO_URL` specified in the configuration section and execute all defined functions in sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Downloading video: https://www.bilibili.com/video/BV1xQ4y167xr/\n",
      "‚úÖ Video downloaded successfully.                           \n",
      ">>> Processing video and extracting frames...\n",
      "‚úÖ Frame extraction complete. Video divided into 10 chunks.\n",
      ">>> Starting concurrent processing of 10 video chunks (Concurrency: 4)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [01:09<00:00,  6.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Aggregating content and generating final code...\n",
      ">>> The model is now generating code:\n",
      "\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "<head>\n",
      "    <meta charset=\"UTF-8\">\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
      "    <title>3D Flipping Cards</title>\n",
      "    <style>\n",
      "        body {\n",
      "            min-height: 100vh;\n",
      "            margin: 0;\n",
      "            padding: 20px;\n",
      "            display: flex;\n",
      "            justify-content: center;\n",
      "            align-items: center;\n",
      "            flex-wrap: wrap;\n",
      "            gap: 20px;\n",
      "            background-color: #1db7c2;\n",
      "            box-sizing: border-box;\n",
      "        }\n",
      "\n",
      "        .card {\n",
      "            position: relative;\n",
      "            width: 100px;\n",
      "            height: 100px;\n",
      "            perspective: 1000px;\n",
      "            margin: 8px 2px;\n",
      "        }\n",
      "\n",
      "        .cover, .back {\n",
      "            position: absolute;\n",
      "            width: 100%;\n",
      "            height: 100%;\n",
      "            display: flex;\n",
      "            justify-content: center;\n",
      "            align-items: center;\n",
      "            backface-visibility: hidden;\n",
      "            border-radius: 8px;\n",
      "            box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);\n",
      "            transition: transform 0.6s ease-in-out;\n",
      "            font-size: 2.5em;\n",
      "        }\n",
      "\n",
      "        .cover {\n",
      "            background-color: rgba(255, 255, 255, 0.5);\n",
      "            transform: rotateY(0deg);\n",
      "        }\n",
      "\n",
      "        .back {\n",
      "            background-color: rgba(253, 183, 194, 0.8);\n",
      "            transform: rotateY(-180deg);\n",
      "        }\n",
      "\n",
      "        .card:hover .cover {\n",
      "            transform: rotateY(180deg);\n",
      "        }\n",
      "\n",
      "        .card:hover .back {\n",
      "            transform: rotateY(0deg);\n",
      "        }\n",
      "    </style>\n",
      "</head>\n",
      "<body>\n",
      "    <div class=\"card\">\n",
      "        <div class=\"cover\">üêµ</div>\n",
      "        <div class=\"back\">üôà</div>\n",
      "    </div>\n",
      "    \n",
      "    <div class=\"card\">\n",
      "        <div class=\"cover\">üò∫</div>\n",
      "        <div class=\"back\">üò≤</div>\n",
      "    </div>\n",
      "    \n",
      "    <div class=\"card\">\n",
      "        <div class=\"cover\">üë¶</div>\n",
      "        <div class=\"back\">üë¥</div>\n",
      "    </div>\n",
      "</body>\n",
      "</html>\n",
      "\n",
      ">>> Workflow finished! Webpage saved as: final_result.html\n",
      ">>> Opening the result page in your default browser...\n",
      ">>> Temporary files have been cleaned up.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"The main execution function that orchestrates the entire workflow.\"\"\"\n",
    "    if \"YOUR_ERNIE_API_KEY\" in API_KEY or not API_KEY:\n",
    "        print(\"‚ùå Error: Please return to the configuration cell (step 4) and set your API_KEY correctly.\")\n",
    "        return\n",
    "\n",
    "    # Initialize the API client\n",
    "    client = OpenAI(api_key=API_KEY, base_url=BASE_URL)\n",
    "    \n",
    "    # Steps 1 & 2: Download and extract frames\n",
    "    video_path = download_video(VIDEO_URL)\n",
    "    if not video_path: return\n",
    "    \n",
    "    chunks = extract_frames(video_path)\n",
    "    if not chunks: return\n",
    "    \n",
    "    # Step 3: Concurrently process all chunks\n",
    "    print(f\">>> Starting concurrent processing of {len(chunks)} video chunks (Concurrency: {MAX_CONCURRENT_REQUESTS})...\")\n",
    "    chunk_summaries = {}\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=MAX_CONCURRENT_REQUESTS) as executor:\n",
    "        future_to_chunk_index = {executor.submit(process_chunk_with_retry, client, i, chunk): i for i, chunk in enumerate(chunks)}\n",
    "        \n",
    "        for future in tqdm(as_completed(future_to_chunk_index), total=len(chunks), desc=\"Processing Chunks\"):\n",
    "            idx, summary = future.result()\n",
    "            if summary:\n",
    "                chunk_summaries[idx] = summary\n",
    "                \n",
    "    if not chunk_summaries:\n",
    "        print(\"\\n‚ùå Failed to process all video chunks. Cannot generate final code.\")\n",
    "        return\n",
    "        \n",
    "    # Step 4: Aggregate content and generate webpage\n",
    "    html_code = aggregate_and_generate_webpage(client, chunk_summaries)\n",
    "    \n",
    "    # Step 5: Save and open the result\n",
    "    output_file = \"final_result.html\"\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(html_code)\n",
    "    \n",
    "    print(f\"\\n\\n>>> Workflow finished! Webpage saved as: {output_file}\")\n",
    "    abs_path = os.path.abspath(output_file)\n",
    "    webbrowser.open(f\"file://{abs_path}\")\n",
    "    print(f\">>> Opening the result page in your default browser...\")\n",
    "    \n",
    "    # Clean up temporary files\n",
    "    shutil.rmtree(\"temp_video\", ignore_errors=True)\n",
    "    print(\">>> Temporary files have been cleaned up.\")\n",
    "\n",
    "# --- Run the main program ---\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Final Result Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"video2code_demo.mp4\" controls  width=\"800\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "# Embed and play local video files\n",
    "Video(\"video2code_demo.mp4\", width=800)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
