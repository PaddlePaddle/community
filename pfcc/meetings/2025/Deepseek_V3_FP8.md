# 简单聊聊 Deepseek V3 的 FP8 训练

## 引子

Deepseek V3 的报告在网上放出之后，在知乎也看了很多训练分析和推理的文章。前段时间也转发了一位大佬的 pp 分析和通信文章，正值赶上前两天没空。29 号凌晨才把文章看完，顺着这股劲儿写一些 Deepseek V3 里 fp8 训练。若有错误请大家指正，也希望和大佬们一起讨论。

FP8 量化是一种新兴的低精度数值表示方法，通过将 FP16 或 FP32 降低到 8 位浮点数，可以显著减少模型的内存占用(相比 FP32 降低 75%)和计算开销，同时提升推理速度和能效比。它提供 E4M3 和 E5M2 两种格式以平衡精度和范围，在保证模型性能的前提下，尤其适合大规模 AI 模型的训练和部署加速。不过要充分发挥 FP8 的优势，需要特定硬件支持，如 NVIDIA Hopper 架构 GPU。

## 整体一览

从 Deepseek V3 的技术报告里写到，它在 Embedding、Attention、gating（MoE 路由）、Norm 上是用的原始精度（BF16 or FP32，某些 gating 操作会上 FP32 吧）。也就是说会做 FP8 的位置基本就在 MoE 上的 MLP 和 Attention 前后的 MLP。

![](https://cdn.nlark.com/yuque/0/2024/png/27816614/1735410875964-7a02908c-4ee3-4e1c-b11d-88dcb50e7bee.png)

技术报告里讲到，会对 FP8 训练进行量化。这也是常规操作了，NV 的一次 OpenDay 也讲了关于 FP8 训练时的量化。具体可看：

[https://www.bilibili.com/video/BV1tM4m117eL/?spm_id_from=333.999.0.0&vd_source=ff8cede52e7c084fcc553a857890b778](https://www.bilibili.com/video/BV1tM4m117eL/?spm_id_from=333.999.0.0&vd_source=ff8cede52e7c084fcc553a857890b778)。

主要的原因是，虽然 fp8 对 scale 不敏感。但是因为 fp32/bf16 -> fp8 的数据可能会落出 fp8 的可表示范围外，造成一定的溢出。所以需要给 fp8 量化带上一个 scale。

## 量化方式

在做大模型推理时，常用的量化方式有 per tensor 量化、per token(逐行)量化、groupwise 量化、tile wise 量化这四种量化方式分别对应的是

1. per tensor：对一个 tensor（二维）量化成低比特，并用一个 scale 表示。
2. per token：对一行/一列元素量化成低比特，并每一列都用一个 scale 表示。
3. group wise 量化：对特定个元素为一组，每组元素用一个 scale 进行表示。
4. tile wise 量化：对特定的一块区域进行量化，并对这块特定的位置取一个 scale（比如 128x128）

当然，可以理解。量化的成员越进行细分，它解量化恢复精度时的量化误差也会越接近。group wise 对应的量化精度也是最高的。（想起了和前 mentor 探讨不同量化方式的区别时，他和我讲。你选择如何量化完全是一种自由，如果你想。可以用各种诡异的方式进行量化，完全取决于硬件特性是否能在你的量化算法上拿到收益。）

话说回来，对应 tile wise 的量化方法。最近 Sage Attention V2 的对 attention 的量化方式，就使用了分小块进行量化的方式。

![画板](https://cdn.nlark.com/yuque/0/2024/jpeg/27816614/1735411886753-9d8b5056-879f-46e2-94b5-85458799c584.jpeg)

讲了推理用到的量化方式之后，对应 Deepseek V3 FP8 训练。作者在文章中提到使用了 groupwise 和 tilewise 的方法进行量化。

![](https://cdn.nlark.com/yuque/0/2024/png/27816614/1735412233647-2aa1a1bc-a654-4001-bc23-938d18fb5bd5.png)

tr 对输入的 input 按 groupwise 进行量化，权重按 tilewise 进行量化。分别拿到了输入和权重的 scale，在 tensorcore 上对一条(1x128) 和 一块(128x128)进行 fp8 矩阵乘法之后。拿回到 cudacore 上并同时乘上 scale 进行解量化。之后的操作就是 loop 进行矩阵乘法。

在这里量化的部分或许会带来一些性能问题，这个也在后续对硬件的改进建议里 Deepseek 提到了。那就是在每一个块进行 wgmma 计算时，都会有 scale 的加载问题。也就是说在 loop 的过程中，会不断的加载 input 和 weight 的 scale 来进行解量化流程。当计算量变大时，不断的解量化（因为在 tensorcore->cudacore 上的搬移）带来的开销或许会抹平 FP8 拿到的量化收益。（NV 也说在下一代 tensorcore 上会有更细粒度的量化方式，期待一下吧）

当然，我们也并不需要每次都是 1x128 128x128 -> 1x128 这样之后再解量化。像下图里的方式也是可行的，这样也更方便利用 tensorcore 进行 wgmma。

![画板](https://cdn.nlark.com/yuque/0/2024/jpeg/27816614/1735415191774-2ebf20cb-4db0-4a41-8874-9d843acac33f.jpeg)

## 使用 CUDA Core 进行累加

<font style="color:rgba(0, 0, 0, 0.8);">WGMMA (Warpgroup MMA) 是 NVIDIA Hopper 架构中引入的一种新的指令集或技术，主要用于支持异步计算以及直接读取共享内存（SMEM）中的数据进行计算，从而提升 GPU 在特定任务中的性能表现</font>

tr 中发现 NVIDIA H800 GPU 上 FP8 GEMM 的累加精度仅能保持约 14 位，这明显低于 FP32 累加精度。并且默认在 tensorcore 上进行 fp8 累加是默认选项。deepseek 的做法是，做一段 wgmma 之后，在 CUDA Core 中进行累加。文章中说开的 wgmma 数量为 4 之后进行累加

![画板](https://cdn.nlark.com/yuque/0/2024/jpeg/27816614/1735414776613-c66c946b-4955-41c6-a1d8-a0e3bf1bcef4.jpeg)

盲猜切分方法为：

```latex
// 使用.m64n128k32的情况
WGMMA_1: A[0:63][0:31] × B[0:31][0:127]     // K维度第1段
WGMMA_2: A[0:63][32:63] × B[32:63][0:127]   // K维度第2段
WGMMA_3: A[0:63][64:95] × B[64:95][0:127]   // K维度第3段
WGMMA_4: A[0:63][96:127] × B[96:127][0:127] // K维度第4段
```

然后把上面这四段统一运进 CUDA Core 上进行累加，累加之后的结果再采用刚才所画的形式进行解码。

在 H800 架构上，通常有两个 WGMMA 同时存在。也就是说在一个 group 上进行矩阵乘法，另一个 group 上进行累加。这样可以保持张量核心的高利用率。

## 整体量化流程

![](https://cdn.nlark.com/yuque/0/2024/png/27816614/1735415399145-d847697a-200a-4884-b251-b99d9e712fc1.png)

看完了量化的方式之后，我们来看一看 FP8 的整体流程。

1. 前向：在输入采用 BF16，主权重采用 FP32，量化到 FP8。前向在累加之后是 FP32 的输出，cast 到 BF16.
2. 反向：wgrad 累加之后天然为 FP32 的 dtype，优化器状态采用 BF16。weight 以 FP32 进行更新。对于 dgrad 来说，FP8 反向之后 cast 成 BF16 继续向前传播。

## FP8 Dtype 选取

FP8 支持两种 Dtype，一种是 e4m3。这种方式具有更精确的数值，但是较小的动态范围。一种方式是 e5m2，这种方式有较大的范围。但是数值不如 e4m3 精确。在 Deepseek 的 FP8 训练里，它保持了较精确的数值。全程使用了 e4m3。

### attention out proj 特殊精度

作者提到在 attention 的输出时，它的反向会对 attention 有较高的精度影响。作者把它提升到 e5m6。（有一说一并不是很明白做一个奇怪的中间精度的意义，out proj 公认的不好处理。转回 bf16 算不就好了嘛。。。前向的时候 attention 也要 bf16，还不用量化。反向的时候就 cast 一下就好了）

## 量化选择

在 per tensor 的框架中采用了延迟量化，这种方法会保留先前若干次迭代中的最大绝对值历史数据，以推断当前值。也就是一种类似离线量化的手段，这里 deepseek 使用的是在线量化。也就是每次量化前统计量化范围里的 max 值并计算 scale。

## 特殊 tile wise 量化为反向带来的问题

虽然我们的基于 tile 的细粒度量化有效地减轻了特征异常值带来的误差，但它在激活量化时需要不同的分组方式：前向传播时是 1x128，反向传播时是 128x1。激活梯度也需要类似的处理。一个直观的策略是像量化模型权重那样，对每个 128x128 的元素进行块级量化。这样，反向传播时只需要进行转置操作。

原因是在处理 weight grad 的时候：

$ grad_weight = input^T × grad_output $

那 input 就变成了之前 transpose 之后的样子，但是对于输入来说。我们统计 scale 还是需要按行进行统计的，所以反向算 weight 的时候需要统计一次不同的 scale。增加了时间。

作者考虑到这个问题，考虑是否可以通过使用相同的 scale 避免这个问题。但是通过对比实验发现 input 对块级量化极为敏感，他们推测不同 token 之间的激活梯度非常不平衡，导致了与 token 相关的异常值。这些异常值无法通过块级量化方法得到有效处理。

## Deepseek 对硬件上的期待

有一说一，感觉这几个 NV 估计都做不到（不知道 fp8 训练这条路跑通之后，nv 会不会做一些量化友好的特性上来。

### Tensor Core 中更高的 FP8 GEMM 累加精度：

在当前 NVIDIA Hopper 架构的 Tensor Core 实现中，FP8 GEMM（通用矩阵乘法）采用定点累加，在相加前基于最大指数对尾数乘积进行右移对齐。我们的实验表明，它在符号填充右移后只使用每个尾数乘积的最高 14 位，并截断超出此范围的位。然而，例如，要从 32 个 FP8×FP8 乘法的累加中获得精确的 FP32 结果，至少需要 34 位精度。因此，我们建议未来的芯片设计增加 Tensor Core 的累加精度以支持全精度累加，或根据训练和推理算法的精度要求选择适当的累加位宽。这种方法确保误差在可接受范围内，同时保持计算效率。

### 对 Tile 和 Block 量化的支持：

当前 GPU 仅支持 per tensor 量化，缺乏 tile 和 block 量化等细粒度量化的原生支持。在当前实现中，当达到 NC（128）间隔时，部分结果将从 Tensor Core 复制到 CUDA core，乘以缩放因子，并加到 CUDA core 上的 FP32 寄存器中。虽然结合我们的精确 FP32 累加策略显著减轻了反量化开销，但 Tensor Core 和 CUDA core 之间频繁的数据移动仍然限制了计算效率。因此建议未来的芯片通过使 Tensor Core 能够接收缩放因子并实现带分组缩放的 MMA 来支持细粒度量化。这样，整个部分和累加和反量化可以直接在 Tensor Core 内完成，直到生成最终结果，避免频繁的数据移动。

### 对在线量化的支持：

研究表明在线量化很有效，但当前实现难以有效支持它。在现有流程中，我们需要从 HBM（高带宽内存）读取 128 个 BF16 激活值（前一次计算的输出）进行量化，然后将量化后的 FP8 值写回 HBM，只是为了再次读取用于 MMA。为解决这种低效问题，我们建议未来芯片将 FP8 转换和 TMA（张量内存加速器）访问集成为单个融合操作，这样量化可以在激活从全局内存传输到共享内存期间完成，避免频繁的内存读写。我们还建议支持 warp 级别的转换指令以加速，这进一步促进了层归一化和 FP8 转换的更好融合。另外，可以采用近内存计算方法，将计算逻辑放置在 HBM 附近。在这种情况下，BF16 元素可以在从 HBM 读入 GPU 时直接转换为 FP8，将片外内存访问减少约 50%。

### 对转置 GEMM 操作的支持：

当前架构使矩阵转置与 GEMM 操作的融合变得繁琐。在我们的工作流程中，前向传播期间的激活被量化为 1x128 FP8 tile 并存储。在反向传播期间，需要读出矩阵，反量化，转置，重新量化为 128x1 tile，并存储在 HBM 中。为减少内存操作，我们建议未来芯片能够在 MMA 操作前直接从共享内存中转置读取矩阵，适用于训练和推理所需的那些精度。

## 总结

写到结束天也快亮了，deepseek 也是所知的第一家能训出 fp8 loss 的厂，还记得前两年讨论 int8 和 fp8 优劣的时候（那会还没有 H100，没有 fp8 tensorcore）记得看到知乎有个老哥留言，如果 fp8 训练成功走通的话。无缝衔接 fp8 推理一定会很顺滑。那会还在想 fp8 训练遥遥无期。没想到一眨眼就有人能做到了。deepseek 对训练 fp8 量化的观察和解决方法都很有启发，希望能看到 fp8 训练能继续大放异彩（这样我们搞量化的就能转业去 fp8 了罢。
